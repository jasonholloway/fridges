
Scraping accumulates evidence
And from that evidence we split out objects, to which the evidence attaches to

So a product list is evidence, and is retained as evidence
Every time we fetch a piece of evidence
It relates to others, as in a graph of associated bits

Page1 is evidence, Page2 is evidence, Page3 is evidence...
Each piece of evidence is a particular type, and should be handled according to that type (and also some parameter)

ProductPageHandler
has responsibility for making monotonic changes, always leading to a better representation
and to do that, gets to look at neighbouring elements
should it have privileged locked vision? nope
but we do need such when for instance processing sets of items
there can only be one change at a time
such a set needs a single owner
a single machine then would own the set

individual pages could be sought concurrently
but the set itself needs to be one (unless partitioning)
so the ProductSet is a single machine

and multiple Pages feed into it
each a machine on a schedule, perhaps feeding forward only on changes
(if they have access to the previous fetch from a backing store)
(or even just a simple hash in their phase data - one hash per page)

However, how do we know how many pages there are?
the page handlers would have to come alive when more pages are detected
and sleep on quietness. They would multiply by themselves: just need one type.

-----

The condensing set handler would receive novelties as they came in
How would it handle them?

It would have to have access to the full set
Each item would be checked for presence (how to know if an item has disappeared? ordering is absolutely necessary)

Such documents could be stored as binary blobs even in dynamo, keeping them separate from parsable phase data
each phase could then choose to commit arbitrary pages of data to the store
these additions, under arbitrary names, could be added to the transaction

would reading such side-blobs also require causation tracking?
I'm not sure if it would... actually, yes it would
as they will be queued, and must be chugged through in order

so a sideblob store would keep tabs on blobs
(maybe it could compress too, JIT-style, to avoid unnecessary computation in tight loops)

This sideblob store could in fact underly the phase store: though we do want phases to be text (I think)

-----

So the condenser would have access to this data stowed away, the full set of stuff
There's a question over how much info we have clustered into the index of course

Each new item detected would summon a new machine to fetch precise details of that item
Which would then be owned by that machine

-----

But we also want to project general indices, across all scrapers
And so a new Fridge, as detected by the above condenser
And any new attributes detected on that Fridge
Should be relayed somewhere central

Single machines should own individual indices then
The index of all known fridges would be one
But all other attributes would have their own indices, centrally owned by a single machine
Each such machine would own a blob, in which the index is serialized via compression

if not in dynamo, perhaps blobs could belong to S3
but such blobs can't be transactionally stored alongside table data
the s3 object can be re-put, but when will it be available? and what if we fail to store the phase data corresponding to it?
then data inconsistency will be introduced.

the only thing I can think of here is in having distinct objects for each commit,
with some GC scheme removing old ones after a certain period of obsolescence
this would create quite a bit of dead data though

Let's say tables of blobs are simpler sooner

------

And so indices will be condensed per attribute
However - these indices will be in domain specific terms, normalised
The actual interpretation of AO attributes requires mapping tables
That should as a basic minimum include a work queue of questions

Here, surely, we would use SQS or similar
I'm unsure of benefits of implementing the same inside our system
Other than the fun of creating a data structure
I suppose back-pressure could thereby bring the system to a hault, usefully
If we don't service the work queue, nothing will scrape, nothing will update
This would certainly be useful

So, a putative work queue would enqueue manual tasks and questions,
possibly with a fair load of context
We would need a program serving these to us in a list
We would receive the latest, and post back responses

Which machines would own these?
A response of a certain type would update a mapping
Possibly - each requisite kind of mapping would be associated with a machine
each individual Fridge scraper would delegate to the mapper to extract attributes
in fact, FridgeMapper would receive the Fridge
and delegate to separate rules

Would these rules be preprogrammed?
I've been imagining associations addable via the work queue
A fridge of a certain name, might be mapped to certain attributes
If that evidence never changed
We would never be prompted to parse the name again
But if it did, certain mappings would be put under threat
(they would at least be questioned; we wouldn't insist on a full restatement)

Each mapping would have a justification attached
When the underlying evidence changed, we would be presented with this evidence (and its sibling evidence)
And asked to possibly edit our stated associations

These associations would be stored somewhere then
Something would own them
Could each be a machine, registered to keep tabs on certain items?
The ObjectMapper<Fridge> then keeps these in its little index of resolutions

If it's a fridge, and it has these bits of evidence
Then make these assumptions
How do we know when the assumptions are complete?
Because all the evidence is registered as being known about
Each Fridge has various attributes
These are known about

If a scraper sends in a new attribute
Then this needs to be mapped into the world of indices

Like we'll have levels of attribute
If a new outer attribute is found, then an inner attribute may be emitted

I can also imagine multiple levels of normalisation: one attribute could be mapped to another intermediate attribute
Ie the word 'Blue' in a name could suggest a colour
This unverified colour could then be colour-mapped into a precise colour index
The first evidence would create secondary evidence
Which would itself require mapping (maybe with precedence in the queue for the benefits of locality?)

New evidence needs queuing up then, and working through sequentially

Some evidence would be tagged as ignored: acknowledged, but that's it

-------

So, a database of evidence needed, evidentally
The proposal at one time was to store each rule in a separate machine (meaning with its info stored in a separate small row)
With basically a simple, percolating strategy pattern ensuing
Most such rules wouldn't be stateful, and wouldn't require re-saving (I wonder if this is the case of now? probs not...)

The problem with this approach is the lack of indexing at the top:
Each rule would be enumerated, over and over again

But would this be that bad, given the slowness of change in the source data?

They would have to hook into evidence type, definitely
A raw AO:Fridge would be one type of evidence
And such would be split out into increasingly smaller pellets
Our first prompted mapping would be, what to do with this json document? what important bits can you see?

Rules of such would look at certain json paths
Given changes, they would either map to known values, or throw
In which case a new mapping is needed
And it is acceptable to map to ignore

An exception in a mapping has to go to the work queue for resolution
And can only be resolved by updating the rule for that kind of evidence

These rules will themselves be data
Rule machines will have ids matching the evidence type
And their phase data will amount to a type of interpreter and raw parameterisation,
That could even be simple javascript evalling as a simple first

Any such rules would receive the incoming evidence,
And emit findings back into the ether to be magically processed

It is true though that some kind of registry of these machines will ned to be maintained
(or is it?? the driver will look up a machine by name - if it turns out to be a dud, then it will skip past it - in fact it will throw an exception)

----------

What if we change our minds about some mapping in the future?
Indices should handle updates
So every index should be two ways: items can be removed and readded

We would need some way of retriggering scraped data into the evidence pipeline
not so bad if we have a registry of Fridges

----------

So even a product list page is evidence, and could be delegated to well-known mappers
AO:FridgeListFragment

But - there's no reliably stable ordering to the list
so we'd have to capture the entire lot
to work out inclusions and exlusions
though keeping a fridge too long isn't the end of the world

if we have no nice ordered list (this must be a common situation)
then we can do a kind of ttl thing
if we haven't seen any sign of a particular fridge for a bit
it falls out of the index on next frisk

---------

So the page scrapers don't remember what was on the page before
they feed their entire list to the central index

the central index just receives then these full pages
but - the single page scrapers should map? or, well, no...
they provide evidence

but only after it's been unified surely
so the percolation of evidence happens on an object-by-object basis
though a page is evidence
of a certain type, that we want to publish into the system of handlers

---------

So stateful tracking of a thing requires an object
The overall index of pages is itself an object
We could track each page as an object, if we fancied caching them that way















